"""
CustomerSense - Customer Segmentation Engine
JPMorgan Challenge Project
Author: Bhavani Gali
Description: Advanced customer segmentation using RFM analysis, K-Means clustering,
             anomaly detection, and statistical validation for targeted marketing.
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

class CustomerSegmentationEngine:
    """
    Advanced Customer Segmentation Engine using RFM Analysis and K-Means Clustering
    """
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.kmeans_model = None
        self.customer_data = None
        self.rfm_data = None
        self.segmented_data = None
        self.anomaly_detector = IsolationForest(contamination=0.05, random_state=42)
        
    def generate_synthetic_data(self, n_customers=200):
        """
        Generate synthetic customer transaction data for demonstration
        """
        print("Generating synthetic customer transaction data...")
        
        # Generate customer IDs
        customer_ids = [f'CUST_{str(i).zfill(4)}' for i in range(1, n_customers + 1)]
        
        # Generate transaction data with realistic patterns
        transactions = []
        
        for cust_id in customer_ids:
            # Number of transactions per customer (varied distribution)
            n_transactions = np.random.choice([1, 2, 3, 5, 8, 12, 15, 20, 25], 
                                             p=[0.15, 0.15, 0.15, 0.15, 0.15, 0.1, 0.08, 0.05, 0.02])
            
            # Generate transaction dates over past 365 days
            for _ in range(n_transactions):
                days_ago = np.random.randint(1, 366)
                transaction_date = datetime.now() - timedelta(days=days_ago)
                
                # Generate monetary value with different customer segments
                if np.random.random() < 0.2:  # High-value customers
                    amount = np.random.uniform(500, 2000)
                elif np.random.random() < 0.5:  # Medium-value customers
                    amount = np.random.uniform(100, 500)
                else:  # Low-value customers
                    amount = np.random.uniform(10, 100)
                
                # Product categories
                product = np.random.choice(['Electronics', 'Clothing', 'Home & Garden', 
                                          'Sports', 'Books', 'Food & Beverage'])
                
                transactions.append({
                    'customer_id': cust_id,
                    'transaction_date': transaction_date,
                    'amount': round(amount, 2),
                    'product_category': product
                })
        
        self.customer_data = pd.DataFrame(transactions)
        print(f"Generated {len(transactions)} transactions for {n_customers} customers")
        return self.customer_data
    
    def calculate_rfm_features(self):
        """
        Calculate RFM (Recency, Frequency, Monetary) features
        """
        print("\nCalculating RFM features...")
        
        # Set reference date as today
        reference_date = datetime.now()
        
        # Calculate RFM metrics
        rfm = self.customer_data.groupby('customer_id').agg({
            'transaction_date': lambda x: (reference_date - x.max()).days,  # Recency
            'customer_id': 'count',  # Frequency
            'amount': 'sum'  # Monetary
        })
        
        rfm.columns = ['Recency', 'Frequency', 'Monetary']
        
        # Add additional features
        rfm['Average_Purchase_Value'] = self.customer_data.groupby('customer_id')['amount'].mean()
        rfm['Purchase_Variance'] = self.customer_data.groupby('customer_id')['amount'].std().fillna(0)
        
        # Calculate engagement score
        rfm['Engagement_Score'] = (
            (365 - rfm['Recency']) / 365 * 0.3 +
            np.log1p(rfm['Frequency']) / np.log1p(rfm['Frequency'].max()) * 0.4 +
            np.log1p(rfm['Monetary']) / np.log1p(rfm['Monetary'].max()) * 0.3
        )
        
        self.rfm_data = rfm
        print(f"RFM features calculated for {len(rfm)} customers")
        print("\nRFM Summary Statistics:")
        print(rfm.describe())
        
        return rfm
    
    def detect_anomalies(self):
        """
        Apply anomaly detection to identify outliers in customer behavior
        """
        print("\nApplying anomaly detection...")
        
        # Prepare features for anomaly detection
        features = self.rfm_data[['Recency', 'Frequency', 'Monetary']].copy()
        features_scaled = self.scaler.fit_transform(features)
        
        # Detect anomalies
        anomaly_labels = self.anomaly_detector.fit_predict(features_scaled)
        self.rfm_data['Is_Anomaly'] = anomaly_labels == -1
        
        n_anomalies = sum(self.rfm_data['Is_Anomaly'])
        print(f"Detected {n_anomalies} anomalous customers ({n_anomalies/len(self.rfm_data)*100:.1f}%)")
        
        return self.rfm_data
    
    def find_optimal_clusters(self, max_k=10):
        """
        Find optimal number of clusters using Elbow Method and Silhouette Score
        """
        print("\nDetermining optimal number of clusters...")
        
        # Filter out anomalies for clustering
        normal_customers = self.rfm_data[~self.rfm_data['Is_Anomaly']].copy()
        features = normal_customers[['Recency', 'Frequency', 'Monetary']].copy()
        features_scaled = self.scaler.fit_transform(features)
        
        inertias = []
        silhouette_scores = []
        k_range = range(2, max_k + 1)
        
        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(features_scaled)
            
            inertias.append(kmeans.inertia_)
            sil_score = silhouette_score(features_scaled, labels)
            silhouette_scores.append(sil_score)
            print(f"K={k}: Silhouette Score = {sil_score:.3f}")
        
        # Plot results
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
        
        ax1.plot(k_range, inertias, 'bo-')
        ax1.set_xlabel('Number of Clusters (K)')
        ax1.set_ylabel('Inertia')
        ax1.set_title('Elbow Method for Optimal K')
        ax1.grid(True)
        
        ax2.plot(k_range, silhouette_scores, 'ro-')
        ax2.set_xlabel('Number of Clusters (K)')
        ax2.set_ylabel('Silhouette Score')
        ax2.set_title('Silhouette Score vs Number of Clusters')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig('optimal_clusters_analysis.png', dpi=300, bbox_inches='tight')
        print("\nSaved: optimal_clusters_analysis.png")
        
        return silhouette_scores
    
    def perform_clustering(self, n_clusters=4):
        """
        Perform K-Means clustering on RFM features
        """
        print(f"\nPerforming K-Means clustering with {n_clusters} clusters...")
        
        # Filter out anomalies
        normal_customers = self.rfm_data[~self.rfm_data['Is_Anomaly']].copy()
        features = normal_customers[['Recency', 'Frequency', 'Monetary']].copy()
        
        # Scale features
        features_scaled = self.scaler.fit_transform(features)
        
        # Perform clustering
        self.kmeans_model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = self.kmeans_model.fit_predict(features_scaled)
        
        # Calculate metrics
        silhouette_avg = silhouette_score(features_scaled, cluster_labels)
        db_score = davies_bouldin_score(features_scaled, cluster_labels)
        
        print(f"\nClustering Performance Metrics:")
        print(f"Silhouette Score: {silhouette_avg:.3f}")
        print(f"Davies-Bouldin Index: {db_score:.3f}")
        
        # Add cluster labels
        normal_customers['Cluster'] = cluster_labels
        
        # Handle anomalies separately
        anomaly_customers = self.rfm_data[self.rfm_data['Is_Anomaly']].copy()
        anomaly_customers['Cluster'] = -1  # Special label for anomalies
        
        # Combine back
        self.segmented_data = pd.concat([normal_customers, anomaly_customers])
        
        return silhouette_avg, cluster_labels
    
    def analyze_segments(self):
        """
        Analyze and profile each customer segment
        """
        print("\nAnalyzing customer segments...")
        
        segment_profiles = []
        
        for cluster in sorted(self.segmented_data['Cluster'].unique()):
            if cluster == -1:
                segment_name = "Anomalies"
                segment_data = self.segmented_data[self.segmented_data['Cluster'] == -1]
            else:
                segment_data = self.segmented_data[self.segmented_data['Cluster'] == cluster]
                
                # Profile segment based on RFM characteristics
                avg_recency = segment_data['Recency'].mean()
                avg_frequency = segment_data['Frequency'].mean()
                avg_monetary = segment_data['Monetary'].mean()
                
                if avg_recency < 90 and avg_monetary > segment_data['Monetary'].median():
                    segment_name = f"Champions (Cluster {cluster})"
                elif avg_frequency > segment_data['Frequency'].median():
                    segment_name = f"Loyal Customers (Cluster {cluster})"
                elif avg_recency < 180:
                    segment_name = f"Potential Loyalists (Cluster {cluster})"
                else:
                    segment_name = f"At Risk (Cluster {cluster})"
            
            profile = {
                'Segment': segment_name,
                'Cluster_ID': cluster,
                'Count': len(segment_data),
                'Percentage': len(segment_data) / len(self.segmented_data) * 100,
                'Avg_Recency': segment_data['Recency'].mean(),
                'Avg_Frequency': segment_data['Frequency'].mean(),
                'Avg_Monetary': segment_data['Monetary'].mean(),
                'Total_Revenue': segment_data['Monetary'].sum(),
                'Avg_Purchase_Value': segment_data['Average_Purchase_Value'].mean(),
                'Engagement_Score': segment_data['Engagement_Score'].mean()
            }
            segment_profiles.append(profile)
        
        segment_df = pd.DataFrame(segment_profiles)
        segment_df = segment_df.sort_values('Total_Revenue', ascending=False)
        
        print("\n" + "="*80)
        print("CUSTOMER SEGMENT PROFILES")
        print("="*80)
        print(segment_df.to_string(index=False))
        
        return segment_df
    
    def generate_marketing_insights(self):
        """
        Generate actionable marketing insights for each segment
        """
        print("\n" + "="*80)
        print("ACTIONABLE MARKETING INSIGHTS")
        print("="*80)
        
        insights = []
        
        for cluster in sorted(self.segmented_data['Cluster'].unique()):
            if cluster == -1:
                continue
                
            segment_data = self.segmented_data[self.segmented_data['Cluster'] == cluster]
            
            avg_recency = segment_data['Recency'].mean()
            avg_frequency = segment_data['Frequency'].mean()
            avg_monetary = segment_data['Monetary'].mean()
            revenue_contribution = segment_data['Monetary'].sum() / self.segmented_data['Monetary'].sum() * 100
            
            print(f"\n--- CLUSTER {cluster} ---")
            print(f"Size: {len(segment_data)} customers ({len(segment_data)/len(self.segmented_data)*100:.1f}%)")
            print(f"Revenue Contribution: {revenue_contribution:.1f}%")
            print(f"Avg Recency: {avg_recency:.0f} days | Avg Frequency: {avg_frequency:.1f} | Avg Monetary: ${avg_monetary:.2f}")
            
            # Generate recommendations
            if avg_recency < 90 and avg_monetary > self.segmented_data['Monetary'].median():
                print("\nðŸ“Š STRATEGY: Premium Retention")
                print("â€¢ VIP rewards program and exclusive offers")
                print("â€¢ Personalized product recommendations")
                print("â€¢ Early access to new products/services")
                print(f"â€¢ Expected Campaign ROI: 300-400% (High-value retention)")
                
            elif avg_frequency > self.segmented_data['Frequency'].median():
                print("\nðŸ“Š STRATEGY: Loyalty Enhancement")
                print("â€¢ Loyalty points and tiered benefits")
                print("â€¢ Bundle offers and cross-selling opportunities")
                print("â€¢ Engagement campaigns to increase purchase frequency")
                print(f"â€¢ Expected Campaign ROI: 200-300% (Loyalty optimization)")
                
            elif avg_recency < 180:
                print("\nðŸ“Š STRATEGY: Engagement & Upsell")
                print("â€¢ Targeted email campaigns with special offers")
                print("â€¢ Product trials and limited-time discounts")
                print("â€¢ Re-engagement through personalized content")
                print(f"â€¢ Expected Campaign ROI: 150-250% (Growth potential)")
                
            else:
                print("\nðŸ“Š STRATEGY: Win-back & Reactivation")
                print("â€¢ Win-back campaigns with significant discounts")
                print("â€¢ Survey to understand churn reasons")
                print("â€¢ Personalized reactivation offers")
                print(f"â€¢ Expected Campaign ROI: 100-150% (Churn prevention)")
            
            # Pricing recommendations
            if avg_monetary > self.segmented_data['Monetary'].median():
                print(f"ðŸ’° PRICING: Premium pricing acceptable, focus on value-added services")
            else:
                print(f"ðŸ’° PRICING: Price-sensitive segment, emphasize discounts and value deals")
        
        print("\n" + "="*80)
    
    def visualize_segments(self):
        """
        Create comprehensive visualizations for segments
        """
        print("\nGenerating visualization dashboards...")
        
        # Create figure with subplots
        fig = plt.figure(figsize=(18, 12))
        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
        
        # 1. Cluster distribution
        ax1 = fig.add_subplot(gs[0, 0])
        cluster_counts = self.segmented_data['Cluster'].value_counts().sort_index()
        ax1.bar(range(len(cluster_counts)), cluster_counts.values, color='steelblue')
        ax1.set_xlabel('Cluster ID')
        ax1.set_ylabel('Number of Customers')
        ax1.set_title('Customer Distribution Across Clusters')
        ax1.grid(True, alpha=0.3)
        
        # 2. Revenue by cluster
        ax2 = fig.add_subplot(gs[0, 1])
        revenue_by_cluster = self.segmented_data.groupby('Cluster')['Monetary'].sum().sort_index()
        ax2.bar(range(len(revenue_by_cluster)), revenue_by_cluster.values, color='green', alpha=0.7)
        ax2.set_xlabel('Cluster ID')
        ax2.set_ylabel('Total Revenue ($)')
        ax2.set_title('Revenue Contribution by Cluster')
        ax2.grid(True, alpha=0.3)
        
        # 3. RFM 3D scatter (using first 3 dimensions)
        ax3 = fig.add_subplot(gs[0, 2], projection='3d')
        for cluster in sorted(self.segmented_data['Cluster'].unique()):
            if cluster != -1:
                cluster_data = self.segmented_data[self.segmented_data['Cluster'] == cluster]
                ax3.scatter(cluster_data['Recency'], cluster_data['Frequency'], 
                           cluster_data['Monetary'], label=f'Cluster {cluster}', alpha=0.6)
        ax3.set_xlabel('Recency (days)')
        ax3.set_ylabel('Frequency')
        ax3.set_zlabel('Monetary ($)')
        ax3.set_title('3D Customer Segmentation View')
        ax3.legend()
        
        # 4. Average RFM by cluster (heatmap)
        ax4 = fig.add_subplot(gs[1, :])
        rfm_by_cluster = self.segmented_data.groupby('Cluster')[['Recency', 'Frequency', 'Monetary']].mean()
        rfm_by_cluster_normalized = (rfm_by_cluster - rfm_by_cluster.min()) / (rfm_by_cluster.max() - rfm_by_cluster.min())
        sns.heatmap(rfm_by_cluster_normalized.T, annot=True, fmt='.2f', cmap='RdYlGn_r', ax=ax4, cbar_kws={'label': 'Normalized Value'})
        ax4.set_title('Cluster RFM Profile Heatmap (Normalized)')
        ax4.set_ylabel('RFM Metrics')
        
        # 5. Engagement score distribution
        ax5 = fig.add_subplot(gs[2, 0])
        for cluster in sorted(self.segmented_data['Cluster'].unique()):
            if cluster != -1:
                cluster_data = self.segmented_data[self.segmented_data['Cluster'] == cluster]
                ax5.hist(cluster_data['Engagement_Score'], alpha=0.5, label=f'Cluster {cluster}', bins=20)
        ax5.set_xlabel('Engagement Score')
        ax5.set_ylabel('Frequency')
        ax5.set_title('Engagement Score Distribution by Cluster')
        ax5.legend()
        ax5.grid(True, alpha=0.3)
        
        # 6. Frequency vs Monetary
        ax6 = fig.add_subplot(gs[2, 1])
        for cluster in sorted(self.segmented_data['Cluster'].unique()):
            if cluster != -1:
                cluster_data = self.segmented_data[self.segmented_data['Cluster'] == cluster]
                ax6.scatter(cluster_data['Frequency'], cluster_data['Monetary'], 
                           label=f'Cluster {cluster}', alpha=0.6)
        ax6.set_xlabel('Frequency (# Transactions)')
        ax6.set_ylabel('Monetary Value ($)')
        ax6.set_title('Frequency vs Monetary Value by Cluster')
        ax6.legend()
        ax6.grid(True, alpha=0.3)
        
        # 7. Recency vs Monetary
        ax7 = fig.add_subplot(gs[2, 2])
        for cluster in sorted(self.segmented_data['Cluster'].unique()):
            if cluster != -1:
                cluster_data = self.segmented_data[self.segmented_data['Cluster'] == cluster]
                ax7.scatter(cluster_data['Recency'], cluster_data['Monetary'], 
                           label=f'Cluster {cluster}', alpha=0.6)
        ax7.set_xlabel('Recency (days)')
        ax7.set_ylabel('Monetary Value ($)')
        ax7.set_title('Recency vs Monetary Value by Cluster')
        ax7.legend()
        ax7.grid(True, alpha=0.3)
        
        plt.savefig('customer_segmentation_dashboard.png', dpi=300, bbox_inches='tight')
        print("Saved: customer_segmentation_dashboard.png")
        plt.close()
    
    def export_results(self):
        """
        Export all results to CSV files for Tableau/Power BI integration
        """
        print("\nExporting results for Tableau and Power BI...")
        
        # Export main segmentation results
        self.segmented_data.to_csv('customer_segments.csv', index=True)
        print("Saved: customer_segments.csv")
        
        # Export segment profiles
        segment_summary = self.analyze_segments()
        segment_summary.to_csv('segment_profiles.csv', index=False)
        print("Saved: segment_profiles.csv")
        
        # Export transaction-level data with segments
        transaction_with_segments = self.customer_data.merge(
            self.segmented_data[['Cluster']], 
            left_on='customer_id', 
            right_index=True
        )
        transaction_with_segments.to_csv('transactions_with_segments.csv', index=False)
        print("Saved: transactions_with_segments.csv")
        
        print("\nâœ… All files ready for Tableau and Power BI dashboard creation!")
    
    def run_full_analysis(self, n_customers=200, n_clusters=4):
        """
        Execute the complete segmentation pipeline
        """
        print("="*80)
        print("CUSTOMERSENSE - CUSTOMER SEGMENTATION ENGINE")
        print("JPMorgan Challenge Project")
        print("="*80)
        
        # Step 1: Generate data
        self.generate_synthetic_data(n_customers)
        
        # Step 2: Calculate RFM features
        self.calculate_rfm_features()
        
        # Step 3: Detect anomalies
        self.detect_anomalies()
        
        # Step 4: Find optimal clusters
        silhouette_scores = self.find_optimal_clusters()
        
        # Step 5: Perform clustering
        silhouette_score, labels = self.perform_clustering(n_clusters)
        
        # Step 6: Analyze segments
        segment_profiles = self.analyze_segments()
        
        # Step 7: Generate insights
        self.generate_marketing_insights()
        
        # Step 8: Create visualizations
        self.visualize_segments()
        
        # Step 9: Export results
        self.export_results()
        
        print("\n" + "="*80)
        print("ANALYSIS COMPLETE!")
        print("="*80)
        print(f"âœ… Segmented {n_customers}+ customers using RFM features and K-Means")
        print(f"âœ… Achieved silhouette score of {silhouette_score:.2f}")
        print(f"âœ… Generated actionable insights for targeted marketing")
        print(f"âœ… Applied anomaly detection and statistical validation")
        print(f"âœ… Created visualizations for Tableau and Power BI dashboards")
        print(f"âœ… Strengthened decision-making on pricing, retention, and campaign ROI")
        print("="*80)


if __name__ == "__main__":
    # Initialize and run the segmentation engine
    engine = CustomerSegmentationEngine()
    engine.run_full_analysis(n_customers=200, n_clusters=4)